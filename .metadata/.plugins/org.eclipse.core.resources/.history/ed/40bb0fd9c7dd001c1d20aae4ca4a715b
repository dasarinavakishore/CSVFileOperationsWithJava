import java.io.BufferedReader;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.LinkedHashSet;
import java.util.List;
import java.util.TreeSet;

public class CSVFileReaderFilterOperations {

	public LinkedHashSet<String> readCSVFilesAndRemoveDuplicates(String[] inputFiles) {

		BufferedReader reader = null;
		String eachRow = "";

		LinkedHashSet<String> finalCsvMergedList = new LinkedHashSet<>();

		for (int i = 0; i < inputFiles.length; i++) {
			String csv_file = inputFiles[i];

			try {
				reader = new BufferedReader(new FileReader(csv_file));
				if (i == 1) {
					if ((eachRow = reader.readLine()) != null) {
						// this condition is to ignore header in 2nd file
					}
				}

				while ((eachRow = reader.readLine()) != null) {
					if (finalCsvMergedList.add(eachRow)) {
						// if the row is added only once to list then no duplicate,,else duplicate row
						System.out.println("added once-->: " + eachRow);

						// removing the row which starts with 'String'
						if (eachRow.startsWith("String")) {
							finalCsvMergedList.remove(eachRow);
						}

						// replacing # with empty
						if (eachRow.startsWith("#")) {
							System.out.println("row contained #==>" + eachRow);
							finalCsvMergedList.remove(eachRow);
							String replacedStr = eachRow.replace("#", "");
							System.out.println("replacedRow==>" + replacedStr);
							finalCsvMergedList.add(replacedStr);
						}

						// replacing end ; with 0;
						if (eachRow.endsWith(";;")) {
							System.out.println("row end contained ;==>" + eachRow);
							finalCsvMergedList.remove(eachRow);
							String replacedStr = eachRow.replaceAll("[;]$", "0");
							System.out.println("replacedRow==>" + replacedStr);
							finalCsvMergedList.add(replacedStr);
						}

					} else if (CommonUtil.checkIsNullOrEmpty(eachRow)) {
						System.out.println("-------duplicate row--------" + eachRow);
					}
				}
			} catch (FileNotFoundException e) {
				e.printStackTrace();
			} catch (IOException e) {
				e.printStackTrace();
			} finally {
				if (reader != null) {
					try {
						reader.close();
					} catch (IOException e) {
						e.printStackTrace();
					}
				}
			}

		}

//		Iterator<String> itr = finalCsvList.iterator();
//		while (itr.hasNext()) {
//			String value = itr.next();
//			if (value.startsWith("String")) {
//				itr.remove();
//			}
//		}

		// to print the finalList
//		finalCsvList.forEach(System.out::println);

		return finalCsvMergedList;

	}

	public TreeSet<String> ReadMergedCSVFile() {

		String csvTempFile = "C:\\Users\\robin\\Documents\\Assessment\\temp\\temp.csv";

		BufferedReader reader = null;
		String eachRow = "";

		TreeSet<String> csvRows = new TreeSet<String>();
		LinkedHashSet<String[]> allCountriesList = new LinkedHashSet<String[]>();
		String headers[] = null;

		List<String[]> finalList = new ArrayList<String[]>();

		try {
			reader = new BufferedReader(new FileReader(csvTempFile));

			if ((eachRow = reader.readLine()) != null) {
				// this condition is for header
				headers = eachRow.split(",");
			}

			while ((eachRow = reader.readLine()) != null) {
				if (csvRows.add(eachRow)) {
					// if the row is added only once to list then no duplicate,,else duplicate row
					// System.out.println(Arrays.toString(eachRow.split(";")));
				} else if (CommonUtil.checkIsNullOrEmpty(eachRow)) {
					System.out.println("-------duplicate row--------" + eachRow);
				}
			}
		} catch (FileNotFoundException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			if (reader != null) {
				try {
					reader.close();
				} catch (IOException e) {
					e.printStackTrace();
				}
			}
		}

		return csvRows;

	}

}
